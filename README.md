# Early Stopping algorithms
This repository is uploaded as a result of the master's thesis *Un Enfoque Integrado para Mejorar la Eficiencia
de la Detección de Eventos P300 para BCIs online*.

TODO: Publish doc link here

# Terms' dictionary
If you came straight from reading the master's thesis here some useful terms translation (from Spanish to English) are provided:
- *Muestra* -> Sample
- *Época* -> Epoch
- *Intento* -> Trial
- *Lote* -> Run
- *Sesión* -> Session
- *Estímulo* -> Stimulus or flash (here both terms are used indistinguishably).

# Code documentation
Every script and function has its own header with a brief description of its functionality, inputs, and outputs. We suggest exploring the code and the main modules in `utils/` to get familiarised with its structure.

# Data organization
This work is mostly based on Ulrich Hoffmann et. al. work: Hoffmann, Ulrich, Jean-Marc Vesin, Touradj Ebrahimi, and Karin Diserens. ‘An Efficient P300-Based Brain–Computer Interface for Disabled Subjects’. Journal of Neuroscience Methods 167, no. 1 (January 2008): 115–25. https://doi.org/10.1016/j.jneumeth.2007.03.005

We recommend to familiarise with the original data's structure and the experiment performed by the authors, the data can be downloaded from here: TODO publish data link here

# General data formatting
## General data structures
In general we work with four data structures:
1. `X`, this structure contains all the electrodes' data.
2. `y`, this structure contains the labels annotated. But please note that **these labels do not necessarily correspond with P300s**! This is because we assumed a P300 was generated by the target stimulus, but this could not be true, i.e. we did not performed a visual search of P300s.
3. `stim_seq`, this structure contains the sequence of stimulus flashed (specifically their indexes). Please note that the stimulus flash order is random in every trial to avoid subject's adaptation affect on the P300s amplitude.
4. `stim_tgt`, this structure is usually composed of a single index which corresponds to the target stimuli for a specific run.

## Arrays shapes
> **Preliminar advice**
> This may be a bit daunting if you see this for the first time, I strongly suggest thinking about this as cubes of data in which every axis represents a specific dimension (except for the original format which is not possible to be seen as such, is composed of 5 dimensions afterall O_o).

There are four data formats employed along all the scripts:
1. The original one: this structure is used just to load the data, afterwards a reshape is performed to have the so-called *regular* data structure, this original shape is:
    - For data (`X`): `(n_runs, n_electr, n_trials, n_flashes, n_timesteps)`
    - For labels and sequences (`y` and `stim_seq`): `(n_runs, n_trials, n_flashes)`
2. The regular one: this structure is used as the starting point to obtain the rest of shapes needed for different purposes. In particular we introduce new dimensions as the combination of others: `n_epochs=n_trials x n_flashes` and `n_features=n_electr x n_timesteps`, to avoid bad indexing issues the axes are swapped accordingly as done in `data_utils.__data_reshape`. In brief, the shapes are:
    - For data (`X`): `(n_runs, n_epochs, n_features)`.
    - For labels and sequences (`y` and `stim_seq`): `(n_runs, n_epochs)`.
3. The SkLearn one: this structure is used to use Scikit-Learn's `LinearDiscriminantAnalysis` class, as the *regular* shape has both runs and epochs we concatenate them within a single axis called `n_samples=n_runs x n_epochs`, and we do not have to worry about the time series because we already concatenated the `n_electr` and `n_timesteps` axes within `n_features`, in brief the shape is:
    - For data (`X`): `(n_samples, n_features)`.
    - For labels (`y`): `(n_samples, )`.
    - For sequences (`stim_seq`): Unused
4. The sorted one: one characteristic of the oddball paradigm employed in Hoffmann et. al. experiments is that the stimulus (a.k.a flashes) are randomised (that's why we have the `stim_seq` structure with the order of these sequences) so we need to sort them accumulate the appropriate evidences to detect the P300-ERP, to do so we start from the classifier's outputs (`(n_samples, )`) and reshape them into sequences by unwrapping the number of epochs & runs, and then the number of trials & flashes. In brief:
    - For data (`X`): Unused
    - For labels (`y`): `(n_runs, n_trials, n_flashes)`
    - For sequences (`stim_seq`): Unused

# Code hierarchy
The code is divided into the following folders:
- `Baseline/`: TODO explain a bit
    - TODO: Briefly explain every script
- `Fixed Parameters/`: TODO explain a bit
    - TODO: Briefly explain every script
- `First Coincidence/`: TODO explain a bit
    - TODO: Briefly explain every script
- `Gain - Loss balance/`: TODO explain a bit
    - TODO: Briefly explain every script
- `scripts/`: TODO explain a bit
    - `LDA`: TODO explain a bit
        - TODO: Briefly explain every script
    - `Plots`: TODO explain a bit
        - TODO: Briefly explain every script
    - `Preprocessing`: TODO explain a bit
        - TODO: Briefly explain every script
- `execute_all_methods.sh`: TODO explain a bit

# Environment
The original environment used to execute this code was:
- OS: Arch Linux - EndeavourOS (64b)
- Python: 3.10.14
- Packages: they can be found within the `requirements.txt` file.
