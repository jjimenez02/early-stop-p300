# Early Stopping algorithms
This repository is uploaded as a result of the master's thesis *Un Enfoque Integrado para Mejorar la Eficiencia
de la Detección de Eventos P300 para BCIs online*.

TODO: Publish doc link here

# Terms' dictionary
If you came straight from reading the master's thesis here some useful terms translation (from Spanish to English) are provided:
- *Muestra* -> Sample
- *Época* -> Epoch
- *Intento* -> Trial
- *Lote* -> Run
- *Sesión* -> Session
- *Estímulo* -> Stimulus or flash (here both terms are used indistinguishably).

# Code documentation
Every script and function has its own header with a brief description of its functionality, inputs, and outputs. We suggest exploring the code and the main modules in `utils/` to get familiarised with its structure.

# Data organization
This work is mostly based on Ulrich Hoffmann et. al. work: Hoffmann, Ulrich, Jean-Marc Vesin, Touradj Ebrahimi, and Karin Diserens. ‘An Efficient P300-Based Brain–Computer Interface for Disabled Subjects’. Journal of Neuroscience Methods 167, no. 1 (January 2008): 115–25. https://doi.org/10.1016/j.jneumeth.2007.03.005

We recommend to familiarise with the original data's structure and the experiment performed by the authors, the data can be downloaded from here: TODO publish data link here

# General data formatting
## General data structures
In general we work with four data structures:
1. `X`, this structure contains all the electrodes' data.
2. `y`, this structure contains the labels annotated. But please note that **these labels do not necessarily correspond with P300s**! This is because we assumed a P300 was generated by the target stimulus, but this could not be true, i.e. we did not performed a visual search of P300s.
3. `stim_seq`, this structure contains the sequence of stimulus flashed (specifically their indexes). Please note that the stimulus flash order is random in every trial to avoid subject's adaptation affect on the P300s amplitude.
4. `stim_tgt`, this structure is usually composed of a single index which corresponds to the target stimuli for a specific run.

## Arrays shapes
> **Preliminar advice**
> This may be a bit daunting if you see this for the first time, I strongly suggest thinking about this as cubes of data in which every axis represents a specific dimension (except for the original format which is not possible to be seen as such, is composed of 5 dimensions afterall O_o).

There are four data formats employed along all the scripts:
1. The original one: this structure is used just to load the data, afterwards a reshape is performed to have the so-called *regular* data structure, this original shape is:
    - For data (`X`): `(n_runs, n_electr, n_trials, n_flashes, n_timesteps)`
    - For labels and sequences (`y` and `stim_seq`): `(n_runs, n_trials, n_flashes)`
2. The regular one: this structure is used as the starting point to obtain the rest of shapes needed for different purposes. In particular we introduce new dimensions as the combination of others: `n_epochs=n_trials x n_flashes` and `n_features=n_electr x n_timesteps`, to avoid bad indexing issues the axes are swapped accordingly as done in `data_utils.__data_reshape`. In brief, the shapes are:
    - For data (`X`): `(n_runs, n_epochs, n_features)`.
    - For labels and sequences (`y` and `stim_seq`): `(n_runs, n_epochs)`.
3. The SkLearn one: this structure is used to use Scikit-Learn's `LinearDiscriminantAnalysis` class, as the *regular* shape has both runs and epochs we concatenate them within a single axis called `n_samples=n_runs x n_epochs`, and we do not have to worry about the time series because we already concatenated the `n_electr` and `n_timesteps` axes within `n_features`, in brief the shape is:
    - For data (`X`): `(n_samples, n_features)`.
    - For labels (`y`): `(n_samples, )`.
    - For sequences (`stim_seq`): Unused
4. The sorted one: one characteristic of the oddball paradigm employed in Hoffmann et. al. experiments is that the stimulus (a.k.a flashes) are randomised (that's why we have the `stim_seq` structure with the order of these sequences) so we need to sort them accumulate the appropriate evidences to detect the P300-ERP, to do so we start from the classifier's outputs (`(n_samples, )`) and reshape them into sequences by unwrapping the number of epochs & runs, and then the number of trials & flashes. In brief:
    - For data (`X`): Unused
    - For labels (`y`): `(n_runs, n_trials, n_flashes)`
    - For sequences (`stim_seq`): Unused

# Code hierarchy
The code is divided into the following folders:
- `Baseline/`: this folder contains everything related with the baseline method against every other early stopping method will be compared. To explain it further, this baseline method consists of a fixed stop strategy at the 20th trial.
- `Fixed Parameters/`: in this directory statistical tests early stopping methods are employed by fixing the statistical test from a selection of tests (Kolmogorov-Smirnov, TTest, Welch's TTest, ...), the significance level $\alpha$ and whether to apply Bonferroni's criteria or not.
    - `stat_test_gain_loss_metrics_script.py`: it defines the statistical test early stopping method with Cross-Validation (it won't optimise any parameter).
    - `acc_evid_stat_test_gain_loss_metrics_script.py`: this script defines the accumulated evidence variant for the statistical test early stopping methods with Cross-Validation (it won't optimise any parameter).
    - `acc_avg_diff_stat_test_gain_loss_metrics_script.py`: this script defines the accumulated averaged differences variant for the statistical test early stopping methods with Cross-Validation (it won't optimise any parameter).
- `First Coincidence/`: directory with a family of early stopping algorithms that optimise their values by fixing the parameter once they succeed for the first time in their prediction.
    - `fixed_stop_optimization_script.py`: early stopping strategy which optimises the number of trials employed.
    - `acc_evid_threshold_optimization_script.py`: scripts which optimises the accumulated evidence threshold.
    - `acc_avg_diff_threshold_optimization_script.py`: early stopping strategy that optimises the accumulated average differences among stimulus.
- `Gain - Loss balance/`: folder containing scripts from the `First Coincidence` and `Fixed Parameters` folders to optimise their parameters trying to balance the `(Gain+Loss)/2` metric.
- `scripts/`: this folder contains some useful scripts to execute the early stopping algorithms.
    - `LDA`: container of the Bayesian Linear Discriminant Analysis' scripts.
    - `Plots`: folder with useful plots to visualize the results.
        - `prepare_data.py`: this script formats the early stopping algorithms outputs to create a single dictionary with all the results.
        - `sbjs_*_plot(s)_script.py`: these scripts plot different visualizations for **each subject** that goes from regular curves, bar-plots and radar/spider plots, we strongly recommend the last visualization.
        - `auc_bar_plots_script.py`: a script that obtains the average and standard deviations for every early stopping result along all subjects to compare the area under the curve of every method (i.e. a global punctuation to know how well it works on all the subjects).
        - `*.sh`: different bash scripts to show some of the early stopping variants results, remember that we have plenty of methods that could be compared, so these scripts just extract a small subset of these methods to ease the comparisons. Please note that the RegExp used to filter the methods were handmade obtained by just considering the algorithms with the highest score in their own folder (i.e. the highest of `First Coincidence` vs the highest of `Fixed Parameters` vs ...), this may look like an arbitrary comparison for you, feel free to modify the RegExps and the scripts to make your own comparisons.
    - `Preprocessing`: directory with the pre-processing routines to apply over the original EEG data.
- `execute_all_methods.sh`: bash script which executes all the early stopping methods, beware of it as it executes in the background most of the early stopping algorithms, i.e. a computer with low resources might crash!
- `outputs.tgz`: a compressed file with the results, this file is provided to avoid the execution of all early stopping algorithms to the user.

# Environment
The original environment used to execute this code was:
- OS: Arch Linux - EndeavourOS (64b)
- Python: 3.10.14
- Packages: they can be found within the `requirements.txt` file.

# Reproduce master thesis
To reproduce the thesis:
1. Open your Linux system, prepare the Python environment, install the requirements, and change your directory to the repository's root folder.
2. Execute all the early stopping strategies by doing `$ ./execute_all_methods.sh` (this will probably take some hours as it will execute 66 early stopping methods within a 4-fold Cross-Validation loop).
    - Alternatively, you can extract the `outputs.tgz` file with `$ tar -xzvf outputs.tgz` within the repository's root directory to avoid executing all the scripts.
3. Gather all the results within a single dictionary by executing `$ python scripts/Plots/prepare_data.py -id . -o out/all_results.pkl`
4. Generate the partial comparisons by executing `$ ./scripts/Plots/plot_all.sh out/all_results.pkl` (this will generate a `Results` folder automatically), this may take some seconds as they are quite a few.
5. Generate the final comparisons by executing `$ ./scripts/Plots/plot_final_comparison.sh out/all_results.pkl Results/Final_Comparisons`

You are done! ;)
